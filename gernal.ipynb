{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adityavb121/23cs503/blob/main/gernal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence tokenization"
      ],
      "metadata": {
        "id": "nAb8jY18HOhp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "66a7b962-08dd-4203-dea8-e7fed07ee43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine Learning is best platform.\n",
            "To group over selves\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Machine Learning is best platform. To group over selves\")\n",
        "for token in doc.sents:\n",
        "  print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word tokenization"
      ],
      "metadata": {
        "id": "dKYJ_1X4HT2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Machine Learning is best platform. To group over selves\")\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "id": "BSCC92-CHIpx",
        "outputId": "44026269-dcb5-4645-cd78-c516221431ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine\n",
            "Learning\n",
            "is\n",
            "best\n",
            "platform\n",
            ".\n",
            "To\n",
            "group\n",
            "over\n",
            "selves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count frequency of words"
      ],
      "metadata": {
        "id": "yYER-4xMHdLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"You only live once. but if you do it right. once is enough.\")\n",
        "word_frequency = Counter()\n",
        "for token in doc:\n",
        "  word_frequency[token.text] += 1\n",
        "  print(word_frequency)"
      ],
      "metadata": {
        "id": "L4NCdunJHV0R",
        "outputId": "b867d533-7a0f-474a-d918-d78920c1f383",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'You': 1})\n",
            "Counter({'You': 1, 'only': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1, 'once': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1, 'once': 1, '.': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1, 'once': 1, '.': 1, 'but': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1, 'once': 1, '.': 1, 'but': 1, 'if': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1, 'once': 1, '.': 1, 'but': 1, 'if': 1, 'you': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1, 'once': 1, '.': 1, 'but': 1, 'if': 1, 'you': 1, 'do': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1, 'once': 1, '.': 1, 'but': 1, 'if': 1, 'you': 1, 'do': 1, 'it': 1})\n",
            "Counter({'You': 1, 'only': 1, 'live': 1, 'once': 1, '.': 1, 'but': 1, 'if': 1, 'you': 1, 'do': 1, 'it': 1, 'right': 1})\n",
            "Counter({'.': 2, 'You': 1, 'only': 1, 'live': 1, 'once': 1, 'but': 1, 'if': 1, 'you': 1, 'do': 1, 'it': 1, 'right': 1})\n",
            "Counter({'once': 2, '.': 2, 'You': 1, 'only': 1, 'live': 1, 'but': 1, 'if': 1, 'you': 1, 'do': 1, 'it': 1, 'right': 1})\n",
            "Counter({'once': 2, '.': 2, 'You': 1, 'only': 1, 'live': 1, 'but': 1, 'if': 1, 'you': 1, 'do': 1, 'it': 1, 'right': 1, 'is': 1})\n",
            "Counter({'once': 2, '.': 2, 'You': 1, 'only': 1, 'live': 1, 'but': 1, 'if': 1, 'you': 1, 'do': 1, 'it': 1, 'right': 1, 'is': 1, 'enough': 1})\n",
            "Counter({'.': 3, 'once': 2, 'You': 1, 'only': 1, 'live': 1, 'but': 1, 'if': 1, 'you': 1, 'do': 1, 'it': 1, 'right': 1, 'is': 1, 'enough': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dropping stop words"
      ],
      "metadata": {
        "id": "89TSxCiZHp4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "stopwords = en.Defaults.stop_words\n",
        "text = \"Machine Learning is best platform. To group over selves\"\n",
        "lst=[]\n",
        "for token in text.split():\n",
        "  if token.lower() not in stopwords:\n",
        "    lst.append(token)\n",
        "#Join items in the list\n",
        "print(\"Original text : \",text)\n",
        "print(\"Text after removing stopwords : \",' '.join(lst))"
      ],
      "metadata": {
        "id": "_Phi3-E6Hre5",
        "outputId": "883c290f-af97-4733-8e6c-d232f0a78a54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text :  Machine Learning is best platform. To group over selves\n",
            "Text after removing stopwords :  Machine Learning best platform. group selves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping punctuations"
      ],
      "metadata": {
        "id": "9iSdY9kaITSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Your input text\n",
        "text = \"This is an example sentence with punctuation, like commas, periods, and hyphens.\"\n",
        "# Process the text using SpaCy\n",
        "doc = nlp(text)\n",
        "# Filter out punctuation tokens\n",
        "filtered_tokens = [token.text for token in doc if not token.is_punct]\n",
        "# Join the filtered tokens to form a clean text without punctuation\n",
        "clean_text = \" \".join(filtered_tokens)\n",
        "print(clean_text)"
      ],
      "metadata": {
        "id": "kp5b0xuKIX6R",
        "outputId": "c1705ad6-929d-4767-dd03-d3c93bd80ffd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example sentence with punctuation like commas periods and hyphens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization\n",
        "\n",
        "Stemming"
      ],
      "metadata": {
        "id": "YZW6RA_GIlEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"\"\"Machine Learning is best platform. To group over selves\"\"\"\n",
        "# Create an instance of the Porter Stemmer from NLTK\n",
        "stemmer = PorterStemmer()\n",
        "# Tokenize the text using spaCy\n",
        "doc = nlp(text)\n",
        "# Perform stemming and print the stemmed words\n",
        "stemmed_words = [stemmer.stem(token.text) for token in doc]\n",
        "print(\" \".join(stemmed_words))"
      ],
      "metadata": {
        "id": "9etMbf0kIte5",
        "outputId": "55da9e7f-dcb4-475e-aac8-93272302326a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "machin learn is best platform . to group over selv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "pyqWMKswI0Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"\"\"Machine Learning is best platform. To group over selves \"\"\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  print(token, token.lemma_)"
      ],
      "metadata": {
        "id": "ERmMZvfEI1Vy",
        "outputId": "ec3b9b52-bcf7-4c3d-a7b7-c2e42e4717e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine machine\n",
            "Learning Learning\n",
            "is be\n",
            "best good\n",
            "platform platform\n",
            ". .\n",
            "To to\n",
            "group group\n",
            "over over\n",
            "selves self\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of Speech Tagging"
      ],
      "metadata": {
        "id": "K_wY8-NNJBQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = nlp(\"Natural Language Toolkit, or more commonly NLTK.\" )\n",
        "for w in text:\n",
        "  print (w, w.pos_)"
      ],
      "metadata": {
        "id": "w4OwRGAoI84a",
        "outputId": "986aa2ae-2099-43b2-bf6e-51922e0206aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural PROPN\n",
            "Language PROPN\n",
            "Toolkit PROPN\n",
            ", PUNCT\n",
            "or CCONJ\n",
            "more ADV\n",
            "commonly ADV\n",
            "NLTK PROPN\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition with SpaCy"
      ],
      "metadata": {
        "id": "BL97Vr2qJKji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"John is looking at buying Macbook startup for $1 billion\")\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "7dMgItXAJIaC",
        "outputId": "93651138-8aa7-4a90-d6c1-b25fc89990da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John 0 4 PERSON\n",
            "Macbook 26 33 ORG\n",
            "$1 billion 46 56 MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorizer"
      ],
      "metadata": {
        "id": "LhkxO_TqJXGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "hat = nlp(\"Apple\")\n",
        "#hat.has_vector\n",
        "hat.vector"
      ],
      "metadata": {
        "id": "muIQegmCJZDr",
        "outputId": "92049dd3-267d-48e4-8b82-f7b005b64e0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1.3281211e+00, -1.0075214e+00,  3.7175745e-01,  9.1043949e-01,\n",
              "        1.9007321e-01, -5.3595680e-01,  7.9103816e-01,  1.1212006e+00,\n",
              "       -6.6392303e-01, -4.5987403e-01,  3.2127166e-01, -2.6990861e-01,\n",
              "       -5.3076094e-01,  9.6939802e-02,  4.7022372e-01,  9.6137977e-01,\n",
              "       -9.8281842e-01, -2.7968022e-01,  5.7331979e-01, -4.8672676e-01,\n",
              "       -6.9897658e-01,  5.4592383e-01, -8.8141191e-01, -2.9142216e-01,\n",
              "       -2.4617831e-01,  1.3789006e-01,  6.9338667e-01,  5.0937206e-02,\n",
              "        5.2165228e-01,  1.5130968e+00, -7.3194104e-01,  5.1477057e-01,\n",
              "        4.9936914e-01,  4.9004722e-01,  8.5975230e-02, -4.6176583e-01,\n",
              "       -5.7331830e-01,  1.2572165e+00,  3.5929558e-01,  2.7258545e-01,\n",
              "       -3.7774915e-01, -3.8565725e-01, -3.5195601e-01, -1.3784751e-01,\n",
              "        4.5454401e-01, -5.2443910e-01, -1.1023723e+00, -1.0151246e+00,\n",
              "        2.7621418e-02, -1.1954986e+00,  5.1338828e-01,  1.3239552e+00,\n",
              "        2.3260400e-02, -4.2658883e-01, -1.4360946e-01, -8.9016998e-01,\n",
              "        1.1097269e+00, -6.4546764e-01, -1.1221370e-01, -2.9117405e-02,\n",
              "       -1.5825453e+00,  2.5865990e-01,  2.9033697e-01, -5.3951681e-01,\n",
              "        8.2261109e-01,  1.1606780e-01,  7.6977575e-01,  1.3088025e-01,\n",
              "       -5.1673807e-02,  9.3487263e-02, -7.8369558e-01,  6.5754533e-01,\n",
              "        1.2913671e+00,  1.3728654e-01, -2.3519650e-01, -8.2473457e-04,\n",
              "        1.5989280e-01,  2.0072207e-02, -7.0021415e-01, -1.6071738e-01,\n",
              "       -4.3503582e-02, -6.9507480e-02, -6.8061161e-01, -9.1045678e-02,\n",
              "        9.4598895e-01,  8.8208342e-01,  2.7235764e-01,  3.7594318e-01,\n",
              "       -4.9334008e-01,  1.1636525e-02, -1.0604942e+00, -3.7983295e-01,\n",
              "        1.4333290e+00, -2.4722154e-01, -4.7444093e-01, -3.8384229e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of n-grams\n",
        "\n",
        "Unigram\n",
        "\n"
      ],
      "metadata": {
        "id": "OFgc6Ud_JhvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I reside in Bengaluru\")\n",
        "grams = ngrams(doc,1)\n",
        "print(list(grams))"
      ],
      "metadata": {
        "id": "4_xEy6XVJd1p",
        "outputId": "0882a3be-81b7-422a-8715-83d478d07edf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(I,), (reside,), (in,), (Bengaluru,)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigrams"
      ],
      "metadata": {
        "id": "Aa7bFIp4Js6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I reside in Bengaluru\")\n",
        "bigrams = ngrams(doc,2)\n",
        "print(list(bigrams))"
      ],
      "metadata": {
        "id": "G0TbJGkIJt4r",
        "outputId": "f516d94d-5fe1-4192-9518-ab63b05d01dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(I, reside), (reside, in), (in, Bengaluru)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigram"
      ],
      "metadata": {
        "id": "_Ud851K2J4RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk import ngrams\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"I reside in Bengaluru\")\n",
        "trigrams = ngrams(doc,3)\n",
        "print(list(trigrams))"
      ],
      "metadata": {
        "id": "KvE4Ldf1Jy16",
        "outputId": "8c0a3348-1efb-496a-c01a-19f173e72293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(I, reside, in), (reside, in, Bengaluru)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of words model"
      ],
      "metadata": {
        "id": "uYVLzOuvKD55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "sent1 = \"India is a republic country. We are proud Indians.\"\n",
        "sent2 = \"The current Prime Minister of India is Shri. Narendra Modi.\"\n",
        "count_vectorizer = CountVectorizer()\n",
        "dtm = count_vectorizer.fit_transform([sent1,sent2])\n",
        "print(pd.DataFrame(data=dtm.toarray(),\n",
        "columns=count_vectorizer.get_feature_names_out()))"
      ],
      "metadata": {
        "id": "okOt5myuKLgu",
        "outputId": "ae8fcdef-cbba-4cd5-b0ed-c17b271536de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   are  country  current  india  indians  is  minister  modi  narendra  of  \\\n",
            "0    1        1        0      1        1   1         0     0         0   0   \n",
            "1    0        0        1      1        0   1         1     1         1   1   \n",
            "\n",
            "   prime  proud  republic  shri  the  we  \n",
            "0      0      1         1     0    0   1  \n",
            "1      1      0         0     1    1   0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WEEK - 10\n",
        "\n",
        "Building deep learning model withTensorFlow and Keras for use cases%load_ext tensorboard"
      ],
      "metadata": {
        "id": "Et3GyAyuLd7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 # Normalize pixel values to bebetween 0 and 1\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "Flatten(input_shape=(28, 28)),\n",
        "Dense(128, activation='relu'),\n",
        "Dense(10, activation='softmax')])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "loss='sparse_categorical_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "# Define the TensorBoard callback\n",
        "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
        "# Train the model with TensorBoard callback\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test,\n",
        "y_test), callbacks=[tensorboard_callback])\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy:',test_acc)\n",
        "%tensorboard --logdir=./logs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4Ij54pJLew6",
        "outputId": "1d7ab06d-5142-46d4-d569-9ddc16cfde00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8752 - loss: 0.4327 - val_accuracy: 0.9559 - val_loss: 0.1455\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9635 - loss: 0.1239 - val_accuracy: 0.9707 - val_loss: 0.0980\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9777 - loss: 0.0767 - val_accuracy: 0.9671 - val_loss: 0.1048\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.0570 - val_accuracy: 0.9736 - val_loss: 0.0858\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9871 - loss: 0.0415 - val_accuracy: 0.9759 - val_loss: 0.0795\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9700 - loss: 0.0939\n",
            "Test accuracy: 0.9758999943733215\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}